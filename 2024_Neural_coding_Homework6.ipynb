{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oxLV72clE080",
    "ExecuteTime": {
     "end_time": "2024-06-21T15:03:51.129602Z",
     "start_time": "2024-06-21T15:03:51.126265Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from IPython.display import display, Math, Latex\n",
    "\n",
    "# Libraries you might need\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Homework 6"
   ],
   "metadata": {
    "id": "H57PQa0nFepR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1\n",
    "\n",
    "Let random variable $X$ be a discrete random variable with values\n",
    "{−2, −1, 0, 1, 2}, obtained with the same probability $p = \\tfrac15$. Let\n",
    "the second random variable $Y$ be $Y = X^2$.\n",
    "\n",
    "(a) Compute analytically $cov(X, Y)$ (2 points)\n",
    "\n",
    "(b) Compute the mutual information between X and Y in bits (2 points).\n",
    "\n",
    "(c) Simulate 100, 1000, 10000 data realization of these processes and com-\n",
    "pute the covariances and mutual information based on the data. (2 points)\n",
    "\n",
    "*+1 bonus point if you compute the mutual information both with a library and yourself with using the formula.*"
   ],
   "metadata": {
    "id": "jQsAp_I-FFMR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answer (a)**\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_i x_i \\cdot P(X = x_i) = \\frac{1}{5} \\cdot (−2 − 1 + 0 + 1 + 2) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[Y] = E[X^2] = \\sum_i x_i^2 \\cdot P(X = x_i) = \\frac{1}{5} \\cdot ((−2)^2 + (−1)^2 + 0^2 + 1^2 + 2^2) = \\frac{1}{5} \\cdot 10 = 2\n",
    "$$\n",
    "\n",
    "$$\n",
    "E[XY] = E[X \\cdot X^2] = E[X^3] = \\sum_i x_i^3 \\cdot P(X = x_i) = \\frac{1}{5} \\cdot ((−2)^3 + (−1)^3 + 0^3 + 1^3 + 2^3) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "cov(X, Y) = E[XY] - E[X]E[Y] = 0 − (0 \\cdot 2) = 0\n",
    "$$\n"
   ],
   "metadata": {
    "id": "KEiVZip4IJV8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answer (b)**\n",
    "\n",
    "$$\n",
    "I(X, Y) = \\sum_{x \\in \\mathcal{A}_X} \\sum_{y \\in \\mathcal{A}_Y} p_{(X, Y)}(x, y) \\cdot log_2 \\frac{p_{(X, Y)}(x, y)}{p_X(x) \\cdot p_Y(y)}\n",
    "$$\n",
    "\n",
    "For each pair $(x, y)$: $p_{(X, Y)}(x, y) = \\frac{1}{5}$\n",
    "\n",
    "Table. Probabilities of $Y$.\n",
    "\n",
    "| $y$ | $P(Y = y)$    |\n",
    "|----|---------------|\n",
    "| 4  | $\\frac{2}{5}$ |\n",
    "| 1  | $\\frac{2}{5}$ |\n",
    "| 0  | $\\frac{1}{5}$ |\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "I(X, Y) &= \\sum_{x \\in \\mathcal{A}_X} \\sum_{y \\in \\mathcal{A}_Y} p_{(X, Y)}(x, y) \\cdot log_2 \\frac{p_{(X, Y)}(x, y)}{p_X(x) \\cdot p_Y(y)} \\\\\n",
    "&= \\frac{1}{5} \\cdot \\big{(} 4 \\cdot log_2 \\frac{ \\frac{1}{5} }{\\frac{2}{5} \\cdot \\frac{1}{5}} + log_2 \\frac{ \\frac{1}{5} }{\\frac{1}{5} \\cdot \\frac{1}{5}} \\big{)} \\\\\n",
    "&= \\frac{1}{5} \\cdot \\big{(} 4 \\cdot log_2 \\frac{5}{2} + log_2\\ 5\\big{)} \\\\\n",
    "&= \\frac{1}{5} \\cdot \\big{(} 4 \\cdot (log_2\\ 5 - log_2\\ 2) + log_2\\ 5\\big{)} \\\\\n",
    "&= \\frac{1}{5} \\cdot \\big{(} 4 \\cdot log_2\\ 5 - 4 + log_2\\ 5\\big{)} \\\\\n",
    "&= \\frac{1}{5} \\cdot \\big{(} 5 \\cdot log_2\\ 5 - 4 \\big{)} \\\\\n",
    "&= log_2\\ 5 - \\frac{4}{5} \\text{[bits]}\n",
    "\\end{split}\n",
    "\\end{equation*}"
   ],
   "metadata": {
    "id": "ne039ma6IpBc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def probability(x, bins):\n",
    "    return np.histogram(x, bins = bins)[0] / len(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:19:46.394712Z",
     "start_time": "2024-06-21T15:19:46.384120Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def joint_probability(x, y, binsx, binsy):\n",
    "    counts = np.histogram2d(x, y, bins = [binsx, binsy])[0].flatten()\n",
    "    return counts[ counts != 0 ] / len(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:19:46.710313Z",
     "start_time": "2024-06-21T15:19:46.704089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "def entropy(probs):\n",
    "    return -np.sum([p * np.log2(p) if p > 0 else 0 for p in probs])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:19:47.197528Z",
     "start_time": "2024-06-21T15:19:47.195139Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def mutual_information(x, y):\n",
    "    binsx = [-2, -1, 0, 1, 2, 3]\n",
    "    binsy = [0, 1, 4, 5]\n",
    "    \n",
    "    px = probability(x, bins = binsx)\n",
    "    entropy_x = entropy(px)\n",
    "\n",
    "    py = probability(y, bins = binsy)\n",
    "    entropy_y = entropy(py)\n",
    "\n",
    "    pxy = joint_probability(x, y, binsx, binsy)\n",
    "    entropy_xy = entropy(pxy)\n",
    "\n",
    "    return entropy_x + entropy_y - entropy_xy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:19:54.920831Z",
     "start_time": "2024-06-21T15:19:54.913430Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "def random_x(n = 1):\n",
    "    return [ random.choice([ -2, -1, 0, 1, 2 ]) for _ in range(n) ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:26:31.528358Z",
     "start_time": "2024-06-21T15:26:31.524604Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N         = 100\n",
      "Cov       = 0.18181818181818182\n",
      "I_formula = 1.4869224242853196\n",
      "I_sklearn = 1.4869224242853205\n",
      "\n",
      "N         = 1000\n",
      "Cov       = -0.03702502502502533\n",
      "I_formula = 1.5173387324484375\n",
      "I_sklearn = 1.517338732448437\n",
      "\n",
      "N         = 10000\n",
      "Cov       = -0.021542484248424816\n",
      "I_formula = 1.5231969968482622\n",
      "I_sklearn = 1.523196996848264\n"
     ]
    }
   ],
   "source": [
    "random.seed(384593)\n",
    "\n",
    "for n_trials in [100, 1000, 10_000]:\n",
    "    x = random_x(n = n_trials)\n",
    "    y = np.power(x, 2)\n",
    "    \n",
    "    print(f\"N         = {n_trials}\")\n",
    "    print(f\"Cov       = {np.cov(x, y)[0][1]}\")\n",
    "    print(f\"I_formula = {mutual_information(x, y)}\")\n",
    "    print(f\"I_sklearn = {sklearn.metrics.mutual_info_score(x, y) / np.log(2)}\")\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-21T15:26:31.776289Z",
     "start_time": "2024-06-21T15:26:31.758495Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "---\n",
    "## Exercise 2\n",
    "Suppose that we have a neuron which, in a given time period, will fire\n",
    "with probability 0.1, yielding a Bernoulli distribution for the neuron’s firing (denoted by the random variable R = 0 or 1) with p(R = 1) = 0.2.\n",
    "\n",
    "(a) Compute the entropy H(R) of this distribution (calculated in bits, i.e., using the base 2 logarithm)? (1 point)\n",
    "\n",
    "(b) Now lets add a stimulus to the picture. Suppose that we think this\n",
    "neuron's activity is related to a light flashing in the eye. Let us say that\n",
    "the light is flashing in a given time period with probability 0.2. Call this\n",
    "stimulus random variable S. If there is a flash, the neuron will fire with\n",
    "probability 1/2. If there is no flash, the neuron will fire with probability\n",
    "1/8. Call the random variable describing whether the neuron fires or not\n",
    "R. Compute the mutual information I(S : R)? (2 points)\\\n",
    "*Hint: First, confirm that H(R) is the same as above by computing p(R).*"
   ],
   "metadata": {
    "id": "ccwv-Yx2HBbn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answer (a)**\n"
   ],
   "metadata": {
    "id": "TG7XOHyVJ9A0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Answer (b)**\n"
   ],
   "metadata": {
    "id": "HIwPtjX2KJUI"
   }
  }
 ]
}
